{
 "metadata": {
  "name": "",
  "signature": "sha256:ad396042275b0159281daea91696b0a4a18fd5f136a4a98da714d9e72a415aad"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.gaussian_process import GaussianProcess\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from KaggleWord2VecUtility import KaggleWord2VecUtility  ## don't remove stopwords\n",
      "from random import randint\n",
      "from sklearn import tree,ensemble\n",
      "from sklearn.feature_selection import SelectKBest, chi2\n",
      "import matplotlib.pyplot as plt\n",
      "%pylab inline\n",
      "from sets import Set\n",
      "from smart_sampling import smartSampling"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING: pylab import has clobbered these variables: ['randint']\n",
        "`%matplotlib` prevents importing * from pylab and numpy\n"
       ]
      },
      {
       "ename": "ImportError",
       "evalue": "No module named smart_sampling",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-d8427b00e4ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'pylab inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msmart_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msmartSampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mImportError\u001b[0m: No module named smart_sampling"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pd.read_csv(os.path.join('labeledTrainData.tsv'), header=0, \\\n",
      "                delimiter=\"\\t\", quoting=3)\n",
      "print 'The first review is:'\n",
      "print data[\"review\"][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Initialize an empty list to hold the clean reviews\n",
      "clean_reviews = []\n",
      "\n",
      "# Loop over each review; create an index i that goes from 0 to the length\n",
      "# of the movie review list\n",
      "print \"Cleaning and parsing the training set movie reviews...\\n\"\n",
      "for i in xrange( 0, len(data[\"review\"])):\n",
      "    clean_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(data[\"review\"][i], True)))\n",
      "print clean_reviews[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nb_reviews = len(clean_reviews)\n",
      "print nb_reviews\n",
      "pop_size = 500\n",
      "print 'keep only',pop_size\n",
      "\n",
      "training_reviews = clean_reviews[:pop_size]\n",
      "test_reviews = clean_reviews[pop_size:(2*pop_size)]\n",
      "\n",
      "training_labels = np.asarray(data[\"sentiment\"][:pop_size])\n",
      "test_labels =  np.asarray(data[\"sentiment\"][pop_size:(2*pop_size)])\n",
      "print training_labels[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scoring_function(parameters):\n",
      "    nb_features,n_estimators = parameters\n",
      "    nb_features = np.int(nb_features)\n",
      "    n_estimators = np.int(n_estimators)\n",
      "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
      "    # bag of words tool.\n",
      "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
      "                             tokenizer = None,    \\\n",
      "                             preprocessor = None, \\\n",
      "                             stop_words = None,   \\\n",
      "                             max_features = nb_features)\n",
      "\n",
      "    # fit_transform() does two functions: First, it fits the model\n",
      "    # and learns the vocabulary; second, it transforms our training data\n",
      "    # into feature vectors. The input to fit_transform should be a list of\n",
      "    # strings.\n",
      "    train_data_features = vectorizer.fit_transform(training_reviews)\n",
      "    ch2 = SelectKBest(chi2, k=final_nb)\n",
      "    ch2.fit(train_data_features,training_labels)\n",
      "    chiSQ_val = ch2.scores_\n",
      "    #print chiSQ_val[:10]\n",
      "    index = np.argsort(chiSQ_val)[::-1]\n",
      "    idx = index[:final_nb]\n",
      "    test_data_features = vectorizer.transform(test_reviews)\n",
      "    \n",
      "    # Numpy arrays are easy to work with, so convert the result to an\n",
      "    # array\n",
      "    train_data_features = train_data_features.toarray()\n",
      "    test_data_features = test_data_features.toarray()\n",
      "    \n",
      "    mean_res = 0.\n",
      "    nb_try = 4\n",
      "    for n_test in range(nb_try): \n",
      "        # Initialize a Random Forest classifier with n_estimators trees\n",
      "        forest = RandomForestClassifier(n_estimators = n_estimators)\n",
      "        forest.fit( train_data_features, training_labels )\n",
      "        predicted_labels = forest.predict(test_data_features)\n",
      "\n",
      "        # Compute accuracy\n",
      "        res = 0\n",
      "        for i in range(len(predicted_labels)):\n",
      "            if (predicted_labels[i] == test_labels[i]):\n",
      "                res += 1\n",
      "        mean_res += (100. * np.float(res))/len(predicted_labels)\n",
      "    \n",
      "    return (mean_res/np.float(nb_try))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Fix parameters of the problem : ####\n",
      "\n",
      "final_nb = 1000 ### the final number of bags kept\n",
      "parameter_bounds = np.asarray( [[3000,15000],[50,1000]] )\n",
      "nb_GCP_steps = 40"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "best_parameter,outputs,outputs_GP = smartSampling(nb_GCP_steps,parameter_bounds,scoring_function,isInt=True,\n",
      "                                                  #corr_kernel= 'squared_exponential',\n",
      "                                                  model = 'both',\n",
      "                                                  nb_random_steps=25, n_clusters=1,returnOutputs=True, verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(1)\n",
      "plt.plot(range(len(outputs)),outputs,'b')\n",
      "plt.plot(range(len(outputs_GP)),outputs_GP,'r')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}